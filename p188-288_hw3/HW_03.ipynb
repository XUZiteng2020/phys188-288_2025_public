{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"HW_03.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PeTCd1kjYPVV"
   },
   "source": [
    "## Homework 3\n",
    "\n",
    "## <em> Linear Algebra - Gaussian Elimination, SVD, Polynomial Regression, PCA, KNN, and Data Modeling</em>\n",
    "\n",
    "### Important Notes Regarding HW3 and Going Forward\n",
    "**From HW3 forward, we will be making use of astro.datahub.berkeley.edu instead of the Standard Datahub.** If you've clicked the nbgitpuller link on the Github page, you should already be taken here automatically.\n",
    "**In astro.datahub, we have our own shared environment which has all relevant packages installed.** In the upper right corner of the notebook, click \"Python\" and make sure you are using **Python(Physics188-288)** as your Jupyter Kernel environment. Otherwise you will not have access to the correct package imports for course assignments.\n",
    "\n",
    "As I have mentioned at the beginning of the semester, DataHub will be the only officially supported place to complete the homeworks. You are free to make use of other Jupyter environments (e.g. locally or on Colab), but those will not be supported and we will not be able to debug those for you. **I highly recommend you make use of DataHub for your homeworks.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4jbgfdiYPVZ"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1830,
     "status": "ok",
     "timestamp": 1727728223491,
     "user": {
      "displayName": "Nathaniel Allen Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "yZ6qUATzYPVa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.integrate import quad\n",
    "import sklearn as sk\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "356opsBlYPVc"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Problem  1 - Solving Least Squares Using Normal Equations and SVD\n",
    "\n",
    "(Reference - NR 15.4) We fit a set of 50 data points $(x_i, y_i)$ to a polynomial $y(x) = a_0 + a_1x + a_2x^2 + a_3x^3$. (Note that this problem is linear in $a_i$ but nonlinear in $x_i$). The uncertainty $\\sigma_i$ associated with each measurement $y_i$ is known, and we assume that the $x_i$'s are known exactly. To measure how well the model agrees with the data, we use the chi-square merti function: <br>\n",
    "\n",
    "$$ \\chi^2 = \\sum_{i=0}^{N-1} \\big( \\frac{y_i-\\sum_{k=0}^{M-1}a_k x^k}{\\sigma_i} \\big)^2. $$\n",
    "\n",
    "<br>\n",
    "where N = 50 and M = 4. Here, $1, x, ... , x^3$ are the basis functions.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot data (make sure to include error bars). (Hint - https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.errorbar.html) </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "error",
     "timestamp": 1727728275653,
     "user": {
      "displayName": "Nathaniel Allen Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "-wjmBY38YPVd",
    "outputId": "baeffda9-fffc-4272-cd3a-ad39b13380a9",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Load a given 2D data\n",
    "data = np.loadtxt(\"./Problem1_data.dat\")\n",
    "x = data[:,0]\n",
    "y = data[:,1]\n",
    "sig_y = data[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "aborted",
     "timestamp": 1727392563174,
     "user": {
      "displayName": "Nathaniel Allen Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "ohsXVJPiYPVe",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "# Scatter plot\n",
    "\n",
    "...\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MF53OVerYPVf"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "We will pick as best parameters those that minimize $\\chi^2$.<br><br>\n",
    "First, let $\\bf A$ be a matrix whose $N \\times M$ components are constructed from the $M$ basis functions evaluated at the $N$ abscissas $x_i$, and from the $N$ measurement errors $\\sigma_i$, by the prescription\n",
    "$$ A_{ij} = \\frac{X_j(x_i)}{\\sigma_i} $$\n",
    "<br>where $X_0(x) = 1,\\ X_1(x) = x,\\ X_2(x) = x^2,\\ X_3(x) = x^3$. We call this matrix $\\bf A$ the design matrix.\n",
    "<br><br>\n",
    "Also, define a vector $\\bf b$ of length $N$ by\n",
    "$$ b_i = \\frac{y_i}{\\sigma_i} $$\n",
    "<br>and denote the $M$ vector whose components are the parameters to be fitted ($a_0, a_1, a_2, a_3$) by $\\bf a$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"><i> 2. Define the design matrix A. (Hint: Its dimension should be NxM = 50x4.) Also, define the vector b. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzZeUZkiYPVg",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define A\n",
    "A = ...\n",
    "# Define b\n",
    "b = ...\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "scqefc27YPVh"
   },
   "source": [
    "Minimize $\\chi^2$ by differentiating it with respect to all $M$ parameters $a_k$ vaishes. This condition yields the matrix equation <br>\n",
    "$$ \\sum_{j=0}^{M-1} \\alpha_{kj}a_j = \\beta_k$$\n",
    "<br> where $\\bf \\boldsymbol \\alpha = A^T \\cdot A$ and $\\bf \\boldsymbol \\beta = A^T \\cdot b$ ($\\boldsymbol \\alpha$ is an $M \\times M$ matrix, and $\\boldsymbol \\beta$ is a vector of length $M$). This is the normal equation of the least squares problem. In matrix form, the normal equations can be written as:\n",
    "$$ \\bf \\boldsymbol \\alpha \\cdot a = \\boldsymbol \\beta. $$\n",
    "<br><br>\n",
    "This can be solve for the vector of parameters $\\bf a$ by linear algebra numerical methods.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 3. Define the matrix alpha and vector beta. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1663704690809,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "yGIwiSJlYPVh",
    "outputId": "ee7a96c6-a937-4474-de03-2066fa292f5b",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Transpose of the matrix A\n",
    "A_transpose = ...\n",
    "\n",
    "# alpha matrix\n",
    "alpha = ...\n",
    "# beta vector\n",
    "beta = ...\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "MFTA3bWAYPVi"
   },
   "source": [
    "<span style=\"color:blue\"> <i> 4. We have $ \\bf \\boldsymbol \\alpha \\cdot a = \\boldsymbol \\beta. $ Solve for $\\bf a$ using (1) \"GaussianElimination_pivot\" defined below (2) LU decomposition and forward subsitution and backsubstitution. Plot the best-fit line from both methods on top of the data. </i></span><br>\n",
    "\n",
    "Hint: You can use scipy.linalg.lu to do the LU decomposition. After you do \"L, U = lu(A, permute_l=True),\" print L and U matrices. Note that L is not a lower triangle matrix. Swap rows of L (and B) and make it a lower triangular matrix. And then, solve for y in Ly = B.\n",
    "\n",
    "e.g.\n",
    "If your L matrix is the following:\n",
    "\n",
    "[[ 0.01577114 0.10593754 0.41569921 1. ]\n",
    "[ 0.03323166 -0.04364428 1. 0. ]\n",
    "[ 0.25163705 1. 0. 0. ]\n",
    "[ 1. 0. 0. 0. ]],\n",
    "\n",
    "you can change it to this:\n",
    "\n",
    "[[ 1. 0. 0. 0. ]\n",
    "[ 0.25163705 1. 0. 0. ]\n",
    "[ 0.03323166 -0.04364428 1. 0. ]\n",
    "[ 0.01577114 0.10593754 0.41569921 1. ]]\n",
    "\n",
    "Then, you should also change B from\n",
    "\n",
    "[ 118.53904396 727.88040211 3581.30337095 22023.93157276]\n",
    "\n",
    "to\n",
    "\n",
    "[22023.93157276 3581.30337095 727.88040211 118.53904396]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QOAT8EkbYPVi",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def GaussianElimination_pivot(A, b):\n",
    "\n",
    "    N = len(b)\n",
    "\n",
    "    for m in range(N):\n",
    "\n",
    "        # Check if A[m,m] is the largest value from elements bellow and perform swapping\n",
    "        for i in range(m+1,N):\n",
    "            if A[m,m] < A[i,m]:\n",
    "                A[[m,i],:] = A[[i,m],:]\n",
    "                b[[m,i]] = b[[i,m]]\n",
    "\n",
    "        # Divide by the diagonal element\n",
    "        div = A[m,m]\n",
    "        A[m,:] /= div\n",
    "        b[m] /= div\n",
    "\n",
    "        # Now subtract from the lower rows\n",
    "        for i in range(m+1,N):\n",
    "            mult = A[i,m]\n",
    "            A[i,:] -= mult*A[m,:]\n",
    "            b[i] -= mult*b[m]\n",
    "\n",
    "    # Backsubstitution\n",
    "    x = np.empty(N,float)\n",
    "    for m in range(N-1,-1,-1):\n",
    "        x[m] = b[m]\n",
    "        for i in range(m+1,N):\n",
    "            x[m] -= A[m,i]*x[i]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1663704696603,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "t4ZIBKt4YPVi",
    "outputId": "9e3fa815-92ef-4027-ba94-fb53903af325",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Using the Gaussian elimination with partial pivoting\n",
    "a_GE = ...\n",
    "\n",
    "print('Using Gaussian Elimination:')\n",
    "print('a0 =', a_GE[0], ', a1 =', a_GE[1], ', a2 =', a_GE[2], ', a3 =', a_GE[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1663704698647,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "zXKb5xdsYPVj",
    "outputId": "8c6a2242-4268-4b78-ae5c-f348f5d33af6",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# \"lu\" does LU decomposition with pivot. Reference - https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.lu_factor.html\n",
    "from scipy.linalg import lu\n",
    "\n",
    "def solve_lu_pivot(A, B):\n",
    "    # LU decomposition with pivot\n",
    "    L, U = lu(A, permute_l=True)\n",
    "\n",
    "    N = len(B)\n",
    "\n",
    "    # forward substitution: We have Ly = B. Solve for y\n",
    "    ...\n",
    "\n",
    "    # backward substitution: We have y = Ux. Solve for x.\n",
    "    ...\n",
    "\n",
    "    return ...\n",
    "\n",
    "a_LU = ...\n",
    "\n",
    "print('Using LU Decomposition:')\n",
    "print('a0 =', a_LU[0], ', a1 =', a_LU[1], ', a2 =', a_LU[2], ', a3 =', a_LU[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1663704701298,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "gQxuLi2ZYPVj",
    "outputId": "1db264da-7ab2-4575-874d-5a01c9f43126",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Best-fit line\n",
    "xx = np.linspace(-5, 7, 100)\n",
    "y_fit_GE = ...\n",
    "y_fit_LU = ...\n",
    "\n",
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "\n",
    "# Make sure you plot the data from part 1 and the best-fit line\n",
    "...\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.xlim(-5, 7)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pmxTWNxTYPVk"
   },
   "source": [
    "The inverse matrix $\\bf C = \\boldsymbol \\alpha^{-1}$ is called the covariance matrix, which is closely related to the probable uncertainties of the estimated parameters $\\bf a$. To estimate these uncertainties, we compute the variance associated with the estimate $a_j$. Following NR p.790, we obtain: <br><br>\n",
    "$$ \\sigma^2(a_j) = \\sum_{k=0}^{M-1} \\sum_{l=0}^{M-1} C_{jk} C_{jl} \\alpha_{kl} = C_{jj} $$\n",
    "<br>\n",
    "<span style=\"color:blue\"> <i> 5. Compute the error (standard deviation - square root of the variance) on the fitted parameters using the covariance matrix. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1663704814607,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "n4VF7RcRYPVk",
    "outputId": "b85323a7-f9e6-4533-cfbc-f0fd7c28cd75",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "\n",
    "...\n",
    "\n",
    "print('Error: on a0 =', sigma_a0, ', on a1 =', sigma_a1, ', on a2 =', sigma_a2, ', on a3 =', sigma_a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gdr1VV43YPVk"
   },
   "source": [
    "Now, instead of using the normal equations, we use singular value decomposition (SVD) to find the solution of least squares. Please read Ch. 15 of NR for more details. Remember that we have the $N \\times M$ design matrix $\\bf A$ and the vector $\\bf b$ of length $N$. We wish to mind $\\bf a$ which minimizes $\\chi^2 = |\\bf A \\cdot a - b|^2$.\n",
    "<br><br>\n",
    "Using SVD, we can decompose $\\bf A$ as the product of an $N \\times M$ column-orthogonal matrix $\\bf U$, an $M \\times M$ diagonal matrix $\\bf S$ (with positive or zero elements - the \"singular\" values), and the transpose of an $M \\times M$ orthogonal matrix $\\bf V$. ($\\bf A = USV^{T}$). <br>\n",
    "Let $\\bf U_{(i)}$ and $\\bf V_{(i)}$ denote the columns of $\\bf U$ and $\\bf V$ respectively (Note: We get $M$ number of vectors of length $M$.) $\\bf S_{(i,i)}$ are the $i$th diagonal elements (singular values) of $\\bf S$. Then, the solution of the above least squares problem can be written as:\n",
    "<br>\n",
    "$$ \\bf a = \\sum_{i=1}^M \\big( \\frac{U_{(i)} \\cdot b}{S_{(i,i)}} \\big) V_{(i)}. $$\n",
    "<br><br>\n",
    "The variance in the estimate of a parameter $a_j$ is given by:\n",
    "$$ \\sigma^2(a_j) = \\sum_{i=1}^M \\big( \\frac{V_{ji}}{S_{ii}} \\big)^2 $$\n",
    "<br>\n",
    "and the covariance:\n",
    "$$ \\mathrm{Cov}(a_j, a_k) = \\sum_{i=1}^M \\big( \\frac{V_{ji}V_{ki}}{S_{ii}^2} \\big). $$\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 6. Decompose the design matrix A using SVD. Estimate the parameter $a_i$'s and its variance. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zmi_0aKYPVl",
    "outputId": "f46b9493-8f29-4904-d571-95314d15156f",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Reference - https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.svd.html\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Decompose A\n",
    "# Note: S, in this case, is a vector of length M, which contains the singular values.\n",
    "U, S, VT = svd(A, full_matrices=False)\n",
    "V = VT.T\n",
    "\n",
    "# Solve for a\n",
    "a_from_SVD = ...\n",
    "\n",
    "print('Using SVD:')\n",
    "print('a0 =', a_from_SVD[0], ', a1 =', a_from_SVD[1], ', a2 =', a_from_SVD[2], ', a3 =', a_from_SVD[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKkgBzORYPVl",
    "outputId": "970ab9e4-ca37-4d6d-9e7f-dd2f05050489",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Error on a\n",
    "sigma_a_SVD = ...\n",
    "\n",
    "\n",
    "print('Error: on a0 =', sigma_a_SVD[0], ', on a1 =', sigma_a_SVD[1], ', on a2 =', sigma_a_SVD[2], ', on a3 =', sigma_a_SVD[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "8GB4n7-1YPVl"
   },
   "source": [
    "Suppose that you are only interested in the parameters $a_0$ and $a_1$. We can plot the 2-dimensional confidence region ellipse for these parameters by building the covariance matrix:\n",
    "$$ \\mathrm{C'} =  \\binom{\\sigma({a_0})^2\\ \\ \\ \\ \\ \\ \\mathrm{Cov}({a_0, a_1})}{\\mathrm{Cov}({a_0, a_1}) \\ \\ \\ \\ \\ \\ \\sigma({a_1})^2} $$\n",
    "<br><br>\n",
    "The lengths of the ellipse axes are the square root of the eigenvalues of the covariance matrix, and we can calculate the counter-clockwise rotation of the ellipse with the rotation angle:\n",
    "$$ \\theta = \\frac{1}{2} \\mathrm{arctan}\\Big( \\frac{2\\cdot \\mathrm{Cov}({a_0, a_1})}{\\sigma({a_0})^2-\\sigma({a_1})^2} \\Big) = \\mathrm{arctan}(\\frac{\\vec{v_1}(y)}{\\vec{v_1}(x)}) $$\n",
    "<br>\n",
    "where $\\vec{v_1}$ is the eigenvector with the largest eigenvalue. So we calculate the angle of the largest eigenvector towards the x-axis to obtain the orientation of the ellipse. <br><br>\n",
    "<br>\n",
    "Then, we multiply the axis lengths by some factor depending on the confidence level we are interested in. For 68%, this scale factor is $\\sqrt{\\Delta \\chi^2} \\approx 1.52$. For 95%, it is $\\approx 2.48$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 7. Compute the covariance between $a_0$ and $a_1$. Plot the 68% and 95% confidence region of the parameter $a_0$ and $a_1$. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnJRzD5eYPVm",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib as mpl\n",
    "from numpy.linalg import eigvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3RKaRcAYPVm",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Compute the covariance\n",
    "\n",
    "# Build the covariance matrix\n",
    "CovM = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFsaEwGTYPVm",
    "outputId": "6704232a-d74e-4184-f395-c511ade599fb",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot the confidence region (https://stackoverflow.com/questions/32371996/python-matplotlib-how-to-plot-ellipse)\n",
    "\n",
    "eigvec, eigval, u = np.linalg.svd(CovM)\n",
    "\n",
    "# Semimajor axis (diameter)\n",
    "semimaj = np.sqrt(eigval[0])*2.\n",
    "# Semiminor axis (diameter)\n",
    "semimin = np.sqrt(eigval[1])*2.\n",
    "\n",
    "theta = np.arctan(eigvec[0][1]/eigvec[0][0])\n",
    "\n",
    "# Plot 1-sig confidence region\n",
    "ell = mpl.patches.Ellipse(xy=[a_GE[0], a_GE[1]], width=1.52*semimaj, height=1.52*semimin, angle = theta*180/np.pi, facecolor = 'dodgerblue', edgecolor = 'royalblue', label = '68% confidence')\n",
    "# Plot 2-sig confidence region\n",
    "ell2 = mpl.patches.Ellipse(xy=[a_GE[0], a_GE[1]], width=2.48*semimaj, height=2.48*semimin, angle = theta*180/np.pi, facecolor = 'skyblue', edgecolor = 'royalblue', label = '95% confidence')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "ax.add_patch(ell2)\n",
    "ax.add_patch(ell)\n",
    "\n",
    "\n",
    "# Set bounds for x,y axes\n",
    "bounds = np.sqrt(CovM.diagonal())\n",
    "plt.xlim(a_GE[0]-4*bounds[0], a_GE[0]+4*bounds[0])\n",
    "plt.ylim(a_GE[1]-4*bounds[1], a_GE[1]+4*bounds[1])\n",
    "\n",
    "plt.grid(True)\n",
    "plt.xlabel('$a_0$')\n",
    "plt.ylabel('$a_1$')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SFArMRU-YPVm"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "In lecture, we discussed that we fit the existing data to obtain model parameters in data analysis, while in machine learning we use the model derived from the existing data to make prediction for new data.\n",
    "\n",
    "Next, let us take the given data and do the polynomial regression.\n",
    "\n",
    "First, split the sample into training data and the testing data. Keep 80% data as training data and uses the remaining 20% data for testing.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 8. Often, the data can be ordered in a specific manner, hence shuffle the data prior to splitting it into training and testing samples. (Use https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.shuffle.html) </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lho7wV75YPVn",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(rng_seed)\n",
    "arr = np.arange(50)\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "train_x = ...\n",
    "train_y = ...\n",
    "train_sigy = ...\n",
    "test_x = ...\n",
    "test_y = ...\n",
    "test_sigy = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gkIRWwo8YPVn"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "In the case of polynomial regression, we need to generate polynomial features (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) for preprocessing. Note that we call each term in the polynomial as a \"feature\" in our model, and here we generate features' high-order (and interaction) terms. For example, suppose we set the degree of the polynomial to be 3. Then, the features of $X$ is transformed from $(X)$ to $(1, X, X^2, X^3)$. We can do this transform using PolynomialFeatures.fit_transform(train_x). But fit_transform() takes the numpy array of shape [n_samples, n_features]. So you need to re-define our training set as train_set_prep = train_x[:,np.newaxis] so that it has the shape [40,1].\n",
    "\n",
    "<span style=\"color:blue\"> <i> 9. Define three different polynomial models with degree of 1, 3, 10. (e.g. model = PolynomialFeatures(degree=...) ) Then, fit to data and transform it using \"fit_transform\"  </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "moqhrmVlYPVn",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# e.g.\n",
    "# model = PolynomialFeatures(degree = ...)\n",
    "# X_model = model.fit_transform(train_x[:,np.newaxis])\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "VXg0rF_XYPVn"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Then, do the least squares linear regression. (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit)\n",
    "\n",
    "1. define the object for linear regression: LR = linear_model.LinearRegression()\n",
    "2. Fit the linear model to the training data: LR.fit(transformed x data, y data)\n",
    "3. Define new x samples for plotting: X_sample = np.linspace(-5, 7, 100)\n",
    "4. Transform x sample: X_sample_transform = model.fit_transform(X_sample[:,np.newaxis])\n",
    "4. Predict using the linear model: Y_sample = LR.predict(X_sample_transform)\n",
    "5. Plot the fit: plt.plot(X_sample, Y_sample)\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> <i> 10. Do the linear regression for three different polynomial models defined in Part 9. Plot the fit on top of the training data (Label each curve). </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GF0jkvoNYPVo",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee8T_LyvYPVo",
    "outputId": "36a9b27f-a7ae-4d44-e4cc-d309668ebe79",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.errorbar(train_x, train_y, yerr = train_sigy, fmt='.', label = 'Training')\n",
    "\n",
    "...\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.xlim(-5, 7)\n",
    "plt.ylim(-12, 65)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mpGldlarYPVo"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> <i> 11. Plot the fit on top of the test data (Label each curve). </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4Xj_cd1YPVo",
    "outputId": "6022f9db-379e-4067-c12a-4f82659b1ccf",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make plot\n",
    "plt.figure(figsize = (10, 7))\n",
    "plt.errorbar(test_x, test_y, yerr = test_sigy, fmt='.', label = 'Test')\n",
    "\n",
    "...\n",
    "\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.xlim(-5, 7)\n",
    "plt.ylim(-12, 65)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YropyNSZYPVp"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "You can obtain the estimated linear coefficients using linear_model.LinearRegression.coef_ (http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)\n",
    "\n",
    "\n",
    "<span style=\"color:blue\"> <i> 12. Print the linear coefficients of three polynomial models you used. For the polynomial of degree 10, do you see that high-order coefficients are very small? </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GGjKRGw8YPVp",
    "otter": {
     "tests": [
      "q1.12"
     ]
    },
    "outputId": "681f68b6-7fa9-4f48-e452-0fe91b1d3861",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "print('polynomial of degree 1: ', LR1.coef_)\n",
    "print('polynomial of degree 3: ', LR3.coef_)\n",
    "print('polynomial of degree 10: ', LR10.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9MHNMrIkYPVp"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "h3pn__iCYPVp"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Problem 2 - Applying the PCA Method on Quasar Spectra\n",
    "\n",
    "The following analysis is based on https://arxiv.org/pdf/1208.4122.pdf.\n",
    "<br><br>\n",
    "\"Principal Component Analysis (PCA) is a powerful\n",
    "and widely used technique to analyze data\n",
    "by forming a custom set of “principal component”\n",
    "eigenvectors that are optimized to describe the\n",
    "most data variance with the fewest number of\n",
    "components. With the full set of eigenvectors the data\n",
    "may be reproduced exactly, i.e., PCA is a transformation\n",
    "which can lend insight by identifying\n",
    "which variations in a complex dataset are most\n",
    "significant and how they are correlated. Alternately,\n",
    "since the eigenvectors are optimized and\n",
    "sorted by their ability to describe variance in the\n",
    "data, PCA may be used to simplify a complex\n",
    "dataset into a few eigenvectors plus coefficients,\n",
    "under the approximation that higher-order eigenvectors\n",
    "are predominantly describing fine tuned\n",
    "noise or otherwise less important features of the\n",
    "data.\" (S. Bailey, arxiv: 1208.4122)\n",
    "<br><br>\n",
    "In this problem, we take the quasar (QSO) spectra from the Sloan Digital Sky Survey (SDSS) and apply PCA to them. Filtering for high $S/N$ in order to apply the standard PCA, we select 18 high-$S/N$ spectra of QSOs with redshift 2.0 < z < 2.1, trimmed to $1340 < \\lambda < 1620\\ \\mathring{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "SaXKwMzlYPVp"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "wavelength = np.loadtxt(\"./Problem2_wavelength.txt\")\n",
    "flux = np.loadtxt(\"./Problem2_QSOspectra.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sCEnXRUkYPVq",
    "outputId": "796b971f-793a-47bf-87c5-e2223c1b4d1b"
   },
   "outputs": [],
   "source": [
    "# Data dimension\n",
    "print( np.shape(wavelength) )\n",
    "print( np.shape(flux) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "IwsmEWUlYPVq"
   },
   "source": [
    "In the above cell, we load the following data: wavelength in Angstroms (\"wavelength\") and 2D array of spectra x fluxes (\"flux\").\n",
    "<br><br>\n",
    "We have 824 wavelength bins, so \"flux\" is 18 $\\times$ 824 matrix, each row containing fluxes of different QSO spectra.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 1. Plot any three QSO spectra flux as a function of wavelength. (In order to better see the features of QSO spectra, you may plot them with some offsets.) </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_INy4vmYPVq",
    "outputId": "16ae9f5a-a749-4684-db2d-5b60e53f8d8d",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "COqmZrxCYPVq"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\"Flux\" is the data matrix of order 18 $\\times$ 824. Call this matrix $\\bf X$.\n",
    "<br><br>\n",
    "We can construct the covariance matrix $\\bf C$ using the mean-centered data matrix. First, calculate the mean of each column and subtracts this from the column. Let $\\bf X_c$ denote the mean-centered data matrix.<br><br>\n",
    "$\\bf X_c =\n",
    "    \\begin{bmatrix}\n",
    "        x_{(1,1)} - \\overline{x}_1 & x_{(1,2)} - \\overline{x}_2 & \\dots  & x_{(1,824)} - \\overline{x}_{824} \\\\\n",
    "        x_{(2,1)} - \\overline{x}_1 & x_{(2,2)} - \\overline{x}_2 & \\dots  & x_{(2,824)} - \\overline{x}_{824} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "        x_{(18,1)} - \\overline{x}_1 & x_{(18,2)} - \\overline{x}_2 & \\dots  & x_{(18,824)} - \\overline{x}_{824}\n",
    "    \\end{bmatrix}$\n",
    "<br><br>\n",
    "where $x_{m,n}$ denote the flux of $m$th QSO in $n$th wavelength bin, and $\\overline{x}_k$ is the mean flux in $k$th wavelength bin.\n",
    "<br><br>\n",
    "Then, the covariance matrix is:\n",
    "$\\bf C$ $ = \\frac{1}{N-1}$ $\\bf X_c^T X_c.$ ($N$ is the number of QSOs.)\n",
    "<br><br>\n",
    "<span style=\"color:blue\"><i> 2. Find the covariance matrix C using the data matrix flux. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FNiluBLYPVq",
    "outputId": "36a07864-3854-45e8-b8c2-4180b89c0d34",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "C = ...\n",
    "\n",
    "print(C)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wPuWvdCsYPVr"
   },
   "source": [
    "<span style=\"color:blue\"> <i> 3. Using numpy.linalg, find eigenvalues and eigenvectors of the covariance matrix. Order the eigenvalues from largest to smallest and then plot them as a function of the number of eigenvalues. (Remember that the eigenvector with the highest eigenvalue is the principle component of the data set.)\n",
    "In this case, we find that our covariance matrix is rank-17 matrix, so we only select the first 17 highest eigenvalues and corresponding eigenvectors (other eigenvalues are close to zero). </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZMsCmz5YPVr",
    "outputId": "14c546c6-5c81-4baf-87d9-1e63017b4bdc",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWR9eiDrYPVr",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "eigval, eigvec = ...\n",
    "\n",
    "# Now sort the eigenvalues from highest to lowest\n",
    "eigval_sorted = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pyHhQcE7YPVr",
    "outputId": "c675ef89-6deb-4d9c-93d5-70979dc175a8",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "plt.ylabel('Eigenvalues')\n",
    "plt.xlabel('Component Number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "dYUHjYP3YPVr"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<span style=\"color:blue\"> <i> 4. Plot the first three eigenvectors. These eigenvectors\n",
    "represent the principal variations of the spectra with respect to that mean spectrum. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G1pDhE56YPVr",
    "outputId": "d9508ea0-8b88-45af-d529-b8005a69bb25",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "YuKpTDnQYPVs"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "The eigenvectors indicate the direction of the principal components, so we can re-orient the data onto the new zes by multiplying the original mean-centered data by the eigenvectors. We call the re-oriented data \"PC scores.\" (Call the PC score matrix $\\bf Z$) Suppose that we have $k$ eigenvectors. Construct the matrix of eigenvectors $\\bf V = [v_1 v_2 ... v_k]$, with $\\bf v_i$ the $i$th highest eigenvector. Then, we can get 18 $\\times\\ k$ PC score matrix by multiplying the 18 $\\times$ 824 data matrix with the 824 $\\times\\ k$ eigenvector matrix:\n",
    "<br><br>\n",
    "$$ \\bf Z = X_c V $$\n",
    "<br><br>\n",
    "Then, we can reconstruct the data by mapping it back to 824 dimensions with $\\bf V^T$:\n",
    "<br><br>\n",
    "$$ \\bf \\hat{X} = \\boldsymbol \\mu + Z V^T $$\n",
    "where $\\boldsymbol \\mu$ is the vector of mean QSO flux.\n",
    "<br><br>\n",
    "Now, comparing the original data with the reconstructed data, we can calculate the residuals. Let $\\bf X_{(i)}, \\hat{X}_{(i)}$ denote the rows of $\\bf X, \\hat{X}$ respectively. Remember that the data matrix has the dimension 18 $\\times$ 824, so each row $\\bf X_{(i)}$ corresponding the spectra of one particular QSO. (For example, if you wish to see the QSO spectra in row 7, you can plot $\\bf X_{(7)}$ as a function of wavelength.). Then, we can simply calculate the residual as $\\frac{1}{N} \\sum_{i=1}^N \\bf |\\hat{X}_{(i)} - X_{(i)}|^2$ where $N$ is the total number of QSOs (NOTE: $\\bf |\\hat{X}_{(i)} - X_{(i)}|$ is the magnitude of the difference between two vectors $\\bf \\hat{X}_{(i)}$ and $\\bf X_{(i)}$.)\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 5. First, start with only mean flux value $\\boldsymbol \\mu$ (in this case $\\bf \\hat{X} = \\boldsymbol \\mu, V = 0$) and calculate the residual. Then, do the reconstruction using the first two principal eigenvectors $\\bf V = [v_1 v_2]$ and calculate the residual. Finally, let $\\bf V = [v_1 v_2 ... v_6]$ (the first six principal eigenvectors) and compute the residual. </i></span><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HixYrrk3YPVs",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "# Residual for V=0\n",
    "res0 = ...\n",
    "\n",
    "# Now find V1\n",
    "...\n",
    "res1 = ...\n",
    "\n",
    "# Now find V2\n",
    "...\n",
    "res2 = ...\n",
    "\n",
    "print(res0, res1, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "klsbKKZSYPVs"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<span style=\"color:blue\"> <i> 6. For any two QSO spectra, plot the original and reconstructed spectra using the first six principal eigenvectors. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4OC4u6vCYPVt",
    "outputId": "4cc137cf-22d1-4276-89b2-65d39199af88"
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "fig = plt.figure(figsize = (10,7))\n",
    "i = 0\n",
    "plt.plot(wavelength, flux[i,:], label = 'Original')\n",
    "plt.plot(wavelength, recons2[i,:], label = 'Reconstructed')\n",
    "plt.plot(wavelength, mu[i]*np.ones(824), label = 'mean')\n",
    "plt.legend()\n",
    "plt.xlabel('Wavelength [Angstrom]')\n",
    "plt.ylabel('Flux')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize = (10,7))\n",
    "i = 2\n",
    "plt.plot(wavelength, flux[i,:], label = 'Original')\n",
    "plt.plot(wavelength, recons2[i,:], label = 'Reconstructed')\n",
    "plt.plot(wavelength, mu[i]*np.ones(824), label = 'mean')\n",
    "plt.legend()\n",
    "plt.xlabel('Wavelength [Angstrom]')\n",
    "plt.ylabel('Flux')\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "daiZigMgYPVt"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<span style=\"color:blue\"> <i> 7. Plot the residual as a function of the number of included eigenvectors. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rF9njRxEYPVt",
    "outputId": "aafc26bb-04a5-42dd-fdd0-0c9926240cc1",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "F3CESR1dYPVt"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "In this problem, we only have 18 QSO spectra, so the idea of using PCA may seem silly. We can also use SVD to find eigenvalues and eigenvectors. With SVD, we get $\\bf X_c = USV^T$. Then, the covariance matrix is $\\bf C$ $ = \\frac{1}{N-1}$ $\\bf X_c^T X_c$ $ = \\frac{1}{N-1}$ $\\bf VS^2V^T.$ Then, the eigenvalues are the squared singular values scaled by the factor $\\frac{1}{N-1}$ and the eigenvectors are the columns of $\\bf V$.\n",
    "<br><br>\n",
    "<span style=\"color:blue\"> <i> 8. Find the eigenvalues applying SVD to the mean-centered data matrix $\\bf X_c$. </i></span><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3fuZUcpYPVt",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "# Decompose A\n",
    "# Note: S, in this case, is a vector of length M, which contains the singular values.\n",
    "...\n",
    "\n",
    "# Print Eigenvalues\n",
    "eigval_SVD_sorted = ...\n",
    "print(eigval_SVD_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2.8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hONhp0fYPVu"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Cb-XSIgoYPVu"
   },
   "source": [
    "#### Problem 3 - MNIST\n",
    "\n",
    "Yann LeCun and collaborators collected and processed 70,000 handwritten digits to produce what became known as the most widely used database in ML, called MNIST. In this assignment, we are going to work with this dataset. Each handwritten digit comes in a square image, divided into a 28×28 pixel grid. Every pixel can take on 256 nuances of the gray color, interpolating between white and black, and hence each the data point assumes any value in the set {0,1,…,255}. There are 10 categories in the problem, corresponding to the ten digits.\n",
    "\n",
    "Ever since, the MNIST problem has become an important standard for benchmarking the performance of more sophisticated Machine Learning models. Often times, there are contests for finding a new constellation of hyperparameters and/or model architecture which results in a better accuracy for correctly classifying the digits.\n",
    "\n",
    "In this exercise, we use the PCA method for dimensionality reduction to reduce the MNIST dataset to 10 digits.\n",
    "\n",
    "As mentioned in Problem 2, PCA is a technique for reducing the number of dimensions in a dataset whilst retaining most information. It is using the correlation between some dimensions and tries to provide a minimum number of variables that keeps the maximum amount of variation or information about how the original data is distributed. It does not do this using guesswork but using hard mathematics and it uses something known as the eigenvalues and eigenvectors of the data-matrix. These eigenvectors of the covariance matrix have the property that they point along the major directions of variation in the data. These are the directions of maximum variation in a dataset. Here, we use the scikit-learn implementation of PCA: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "First, load the MNIST data:\n",
    "(Note: Here, we load only 20% of the whole MNIST data - hence, 14,000 digits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Uvq6FZGMYPVu"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data\n",
    "Y = mnist.target\n",
    "\n",
    "X = X.to_numpy()\n",
    "Y = Y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "cdypZP4bYPVu"
   },
   "source": [
    "\"$X$\" contains information about the given MNIST digits. We have a 28x28 pixel grid, so each image is a vector of length 784; we have 70,000 images (digits), so $X$ is a 70,000x784 matrix. \"$Y$\" is a label (0-9; the category to which each image belongs) vector of length 70,000.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 1. Do the following:\n",
    "\n",
    "(1) Randomly shuffle data (i.e. randomize the order)\n",
    "\n",
    "  (Note: The label $Y_1$ corresponds to a vector $X_{1j}$, and even after shuffling, $Y_1$ should still correspond to $X_{1j}$.)\n",
    "  \n",
    "  \n",
    "(2) Select 1/3 of the data. (You are free to work with a larger set of the data, but it will take much longer time to train.)\n",
    "\n",
    "\n",
    "(3) Split data into training and test samples using train_test_split (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Set train_size = 0.8. (80% of $X$ is our training samples.) Print the dimension of training and test samples. </i></span> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2585,
     "status": "ok",
     "timestamp": 1663622504567,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "_jwKe--zYPVv",
    "outputId": "37b0410b-4bcb-4f9d-8888-b515c8403a51",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# shuffle and select only 1/3 of data\n",
    "random_state = check_random_state(rng_seed)\n",
    "...\n",
    "\n",
    "# pick training and test data sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(...)\n",
    "\n",
    "print( np.shape(X_train), np.shape(Y_train) )\n",
    "print( np.shape(X_test), np.shape(Y_test) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Xfz9fu64YPVv"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Many machine learning algorithms are also not scale invariant, and hence we need to scale the data (different features to a uniform scale). All this comes under preprocessing the data. (http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing) PCA is a prime example of when such normalization is important; if the variables are not measured on the same scale, then each principal component can be dominated by a single variable.\n",
    "\n",
    "In this exercise, the MNIST pixel values in images should also be scaled prior to providing the images as an input to PCA. There are three main types of pixel scaling techniques: normalization (scaling pixel to the range 0-1), centering (scale pixel values to have a zero-mean), and standardization (scale pixel values to have a zero-mean and unit-variance).\n",
    "\n",
    "First, let us try normalization. Each pixel contains a greyscale value quantified by an integer between 0 and 255. To standardize the dataset, we normalize the \"$X$\" data in the interval [0, 1].\n",
    "\n",
    "<span style=\"color:blue\"> <i> 2. Normalize the X data (both training and test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taCudQlZYPVv",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = ...\n",
    "X_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZJzRGSy3YPVv",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wVjfdhlRYPVv"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "(1) Define the PCA model with the first 2 principal components:\n",
    "\n",
    "&nbsp; **pca = PCA(n_components=2)**\n",
    "\n",
    "(2) Using \"fit_transform,\" fit the model with the training X data and apply the dimensionality reduction on it.\n",
    "\n",
    "&nbsp; **X_train_PCA = pca.fit_transform(training X data)**\n",
    "\n",
    "(3) With the same model, apply the dimensionality reduction on the test X data.\n",
    "\n",
    "&nbsp; **X_test_PCA = pca.transform(test X data)**\n",
    "\n",
    "<span style=\"color:blue\"> <i> 3. This problem is similar to HW3-Q4-Part3. For both training and test samples, create a scatterplot of the first and second principal component and color each of the different types of digits with a different color. Label each axis (e.g. x-axis: 1st principal component, y-axis: 2nd principal component). How does it compare to the UMAP results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJdCJfTwYPVv",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Your PCA code here\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 983
    },
    "executionInfo": {
     "elapsed": 1141,
     "status": "ok",
     "timestamp": 1663622518135,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "XNDSDC85YPVw",
    "outputId": "282dccc0-f2ad-47fa-f4b9-a96bd26bb318",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Make the figures:\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EdnPEi6sYPVw"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<span style=\"color:blue\"> <i> 4. Select the first three principal components and make 3D scatterplot on the training data. (similar to HW3-Q4-Part5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRsgnBoBYPVw",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "executionInfo": {
     "elapsed": 4493,
     "status": "ok",
     "timestamp": 1663622576210,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "k5ZGWve2YPVw",
    "outputId": "4267fac7-8003-427a-8f1a-606e497f6a9f",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "EpT7J-eCYPVw"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "From the graph we can see the two or three components definitely hold some information, especially for specific digits, but clearly not enough to set all of them apart. There are other techniques, such as UMAP module or t-SNE (t-Distributed Stochastic Neighbouring Entities), which can better reduce the dimensions for visualisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JhjwOUu5YPVw"
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "zOeLwkHzYPVw"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "abcuaIasYPVx"
   },
   "source": [
    "Now, we will introduce K-nearest neighbors (KNN), one of the most widely used machine learning classification techniques. We use scikit-learn implementation of KNN: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "\n",
    "Ideally, we should tune KNN hyperparameters by doing a grid search using k-fold cross validation, but in this exercise we simply use default parameters with n_neighbors = 6.\n",
    "\n",
    "(1) Define the knn classifier\n",
    "\n",
    "&nbsp; **clf = knn(n_neighbors=6)**\n",
    "\n",
    "(2) Fit the model\n",
    "\n",
    "&nbsp; **clf.fit(training X data, training Y/target data)**\n",
    "\n",
    "(3) Get the classification accuracy on the test data\n",
    "\n",
    "&nbsp; **clf.score(test X data, test Y/target data)**\n",
    "\n",
    "<span style=\"color:blue\"> <i> 5. Evaluate the classification accuracy on the test data using a KNN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6364,
     "status": "ok",
     "timestamp": 1663622590084,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "yu1rR4UFYPVx",
    "outputId": "d462632d-7249-4845-cd03-7a219eb88798",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "test_score = ...\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "CbcaI-k8YPVx"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "The above KNN classifier considers all 784 features for each image when making its decisions. What if you do not need that many? It is possible that a lot of those features do not really affect our predictions that much. Or worse, KNN could be considering feature anomalies that are unique to our training data, resulting in overfitting. One way to deal with this is by removing features that aren’t contributing much.\n",
    "\n",
    "Now, suppose you take the first two principal components from PCA and fit your model using those two components.\n",
    "\n",
    "&nbsp; **pca = PCA(n_components=2)**\n",
    "\n",
    "&nbsp; **X_train_PCA = pca.fit_transform(training X data)**\n",
    "\n",
    "&nbsp; **X_test_PCA = pca.transform(test X data)**\n",
    "\n",
    "Now you can take X_train_PCA, along with training Y data, to fit the KNN model and evaluate the classification accuracy.\n",
    "\n",
    "<span style=\"color:blue\"> <i> 6. Evaluate the classification accuracy with different number of PCA componenets. Let N_PCA_component = [2, 10, 25, 50, 100, 200, 400, 700]. Plot classification accuracy vs. number of PCA components. How does it compare to the accuracy in Part 5? Draw a horizontal line for the accuracy with all 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pe8mV68VYPVx",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "sc_all = [] # This is the list of test scores\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "executionInfo": {
     "elapsed": 487,
     "status": "ok",
     "timestamp": 1663622703368,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "bcNTmmnNYPVx",
    "outputId": "04815566-ff2b-4a75-a576-a0ba91ef9bae",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot as a figure:\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3.6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "4qMtow6pYPVy"
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "Instead of pixel normalization, we can also try feature rescaling through standardization (rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one). We can use sklearn.preprocessing.StandardScaler for this job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5_NpAHCiYPVy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Fw7p-b7_YPVz"
   },
   "source": [
    "(1) Define the StandardScaler\n",
    "\n",
    "&nbsp; **sc = StandardScaler()**\n",
    "\n",
    "(2) Fit the training X data and then transform it.\n",
    "\n",
    "&nbsp; **X_train = sc.fit_transform(training X data)**\n",
    "\n",
    "(3) Perform standardization on the test X data.\n",
    "\n",
    "&nbsp; **X_test = sc.transform(test X data)**\n",
    "\n",
    "<span style=\"color:blue\"> <i> 7. Re-load the MNIST data and try standardization on both training and test X data following the above steps. Evaluate the classification accuracy using a KNN classifier. How does it compare to Part 5? **(Note: Here we're looking for both a code and a short written answer)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7878,
     "status": "ok",
     "timestamp": 1663624129934,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "Lj2bE5SPYPVz",
    "outputId": "988d6cb7-884f-4500-a9eb-a376f02d8329",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 58492,
     "status": "ok",
     "timestamp": 1663624195411,
     "user": {
      "displayName": "Nathaniel Leslie",
      "userId": "10554447447930628413"
     },
     "user_tz": 420
    },
    "id": "MtUJNpbOYPVz",
    "outputId": "1297d7de-d6d1-430e-e844-1bc36e4af1ec",
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "test_score = ...\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Upload the .zip file to gradescope!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (Physics188-288)",
   "language": "python",
   "name": "shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "p188_288_hw3",
   "tests": {
    "q1.12": {
     "name": "q1.12",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bb = np.array([0.60083933, 6.57746667, 2.22758574, 11.44381562, 4.42561743, -1.27048881, -1.10920189, 1.54337324, -4.51688953, -2.41002883, -1.94143754, 7.66081051, 8.58068515, -1.96404621, 0.03226013, -5.78879275, 7.74281779, 2.12455553, -3.0870171, -5.21475617, 16.97929868, 14.79378215, 25.4626895, 26.37650366, 7.40991409, 6.03536611, 7.16687459, 24.38340515, 13.38919514, 2.44614498, 3.65643693, 10.74720102, -0.55961163, 12.92589024, 8.81978901, -0.81108112, 9.33835216, -1.81851989, 2.20953418, -2.8096348, 15.09080603, 0.22711891, -0.79068759, 16.68633994, -1.41608589, 20.90822132, 11.86367256, -4.03992656, -1.5455174, 0.02691957])\n>>> np.all(np.isclose(bb, b, atol=0.001, rtol=0.001))\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> A.shape == (50, 4)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> A_transpose.shape == (4, 50)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> beta\narray([  118.53904396,   727.88040211,  3581.30337095, 22023.93157276])",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> alpha_true = np.array([[7.57344292, 15.9581405, 120.838371, 480.208969], [15.9581405, 120.838371, 480.208969, 3207.04253], [120.838371, 480.208969, 3207.04253, 16288.7892], [480.208969, 3207.04253, 16288.7892, 105149.671]])\n>>> np.all(np.isclose(alpha, alpha_true, atol=0.001, rtol=0.001))\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> beta_true = np.array([118.53904396, 727.88040211, 3581.30337095, 22023.93157276])\n>>> np.all(np.isclose(beta, beta_true, atol=0.001, rtol=0.001))\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.4": {
     "name": "q1.4",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> a_LU\narray([-0.0308163 ,  2.66764608,  0.31483927,  0.07945935])",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> a_GE_correct = np.array([-0.0308163, 2.66764608, 0.31483927, 0.07945935])\n>>> np.all(np.isclose(a_GE, a_GE_correct, atol=0.001, rtol=0.001))\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> a_LU_correct = np.array([-0.0308163, 2.66764608, 0.31483927, 0.07945935])\n>>> np.all(np.isclose(a_LU, a_LU_correct, atol=0.001, rtol=0.001))\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.isclose(a_LU, a_GE, atol=0.001, rtol=0.001))\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.5": {
     "name": "q1.5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(sigma_a0, 0.71394189270878, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(sigma_a1, 0.22390274023290194, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.6": {
     "name": "q1.6",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(a_from_SVD[0], -0.03081629537273578, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(sigma_a_SVD[1], 0.22390274023290282, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.7": {
     "name": "q1.7",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(np.sqrt(CovM[0, 0]), 0.713941892708781)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.9": {
     "name": "q1.9",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.all(np.isclose(X_poly3[:, 3], X_poly3[:, 1] ** 3))\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.2": {
     "name": "q2.2",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> C.shape == (824, 824)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.all(np.isclose(C[0, 0], 0.00887242, rtol=0.01, atol=0.01))\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.3": {
     "name": "q2.3",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(eigval_sorted[0].real, 3.5718335976952313, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.5": {
     "name": "q2.5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(res0.real, 7.838200342799575, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(res1.real, 3.360545086468325, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(res2.real, 1.2199052237128785, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2.8": {
     "name": "q2.8",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(eigval_SVD_sorted[0].real, 3.5718336, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(eigval_SVD_sorted[1].real, 1.16921314, atol=0.001, rtol=0.001)\nnp.True_",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.1": {
     "name": "q3.1",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.round((X_train.shape[0] + X_test.shape[0]) / 70000, 2) == 0.33\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.round(Y_test.shape[0] / (Y_test.shape[0] + Y_train.shape[0]), 1) == 0.2\nnp.True_",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> Y_test.shape[0] == X_test.shape[0]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.5": {
     "name": "q3.5",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> test_score >= 0.94\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3.6": {
     "name": "q3.6",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> sc_all[0] <= 0.5\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sc_all[4] >= sc_all[-1]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
